"""Dataset class for tiny detector training.

Loads 40x40 ROI crops from .npz files generated by generate_tiny_dataset.py.
Applies augmentation and normalization to match NanoDet preprocessing.
"""

from pathlib import Path
from typing import Tuple, Optional
import numpy as np
import torch
from torch.utils.data import Dataset


# ImageNet normalization stats (BGR format, matches NanoDet)
MEAN = np.array([103.53, 116.28, 123.675], dtype=np.float32)
STD = np.array([57.375, 57.12, 58.395], dtype=np.float32)


class TinyDroneDataset(Dataset):
    """PyTorch Dataset for 40x40 drone detection crops.

    Data format (from .npz file):
        - images: (N, roi_size, roi_size, 3) uint8 RGB
        - targets: (N, 4) float32 [cx, cy, w, h] normalized to ROI
        - has_drone: (N,) bool indicating positive samples

    Returns:
        image: (3, roi_size, roi_size) float32 normalized tensor
        target: (4,) float32 [cx, cy, w, h]
        has_drone: bool
    """

    def __init__(
        self,
        data_path: str,
        split: str = "train",
        augment: bool = True,
        roi_size: int = 40,
    ):
        """Initialize dataset.

        Args:
            data_path: Path to directory containing train.npz and val.npz.
            split: "train" or "val".
            augment: Apply data augmentation (brightness, contrast).
            roi_size: Expected ROI size (for validation).
        """
        self.data_path = Path(data_path)
        self.split = split
        self.augment = augment and (split == "train")
        self.roi_size = roi_size

        # Load data
        npz_path = self.data_path / f"{split}.npz"
        if not npz_path.exists():
            raise FileNotFoundError(f"Dataset not found: {npz_path}")

        data = np.load(npz_path)
        self.images = data["images"]  # (N, H, W, 3) uint8 RGB
        self.targets = data["targets"]  # (N, 4) float32 [cx, cy, w, h]
        self.has_drone = data["has_drone"]  # (N,) bool

        # Validate
        assert self.images.shape[1] == roi_size, f"Expected roi_size={roi_size}, got {self.images.shape[1]}"
        assert self.images.shape[2] == roi_size, f"Expected roi_size={roi_size}, got {self.images.shape[2]}"
        assert len(self.images) == len(self.targets) == len(self.has_drone)

        print(f"Loaded {split} dataset: {len(self)} samples "
              f"({self.has_drone.sum()} positive, {(~self.has_drone).sum()} negative)")

    def __len__(self) -> int:
        return len(self.images)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, bool]:
        """Get a single sample.

        Returns:
            image: (3, roi_size, roi_size) float32 normalized
            target: (4,) float32 [cx, cy, w, h]
            has_drone: bool
        """
        image = self.images[idx].copy()  # (H, W, 3) uint8 RGB
        target = self.targets[idx].copy()  # (4,) float32
        has_drone = bool(self.has_drone[idx])

        # Apply augmentation
        if self.augment:
            image = self._augment(image)

        # Preprocess to match NanoDet format
        image = self._preprocess(image)

        return (
            torch.from_numpy(image),
            torch.from_numpy(target),
            has_drone,
        )

    def _augment(self, image: np.ndarray) -> np.ndarray:
        """Apply data augmentation.

        Args:
            image: (H, W, 3) uint8 RGB.

        Returns:
            Augmented image (H, W, 3) uint8 RGB.
        """
        image = image.astype(np.float32)

        # Random brightness (±20%)
        if np.random.random() < 0.5:
            factor = np.random.uniform(0.8, 1.2)
            image = image * factor

        # Random contrast (±20%)
        if np.random.random() < 0.5:
            mean = image.mean()
            factor = np.random.uniform(0.8, 1.2)
            image = (image - mean) * factor + mean

        # Random saturation (±20%)
        if np.random.random() < 0.3:
            gray = image.mean(axis=2, keepdims=True)
            factor = np.random.uniform(0.8, 1.2)
            image = gray + (image - gray) * factor

        # Clip and convert back
        image = np.clip(image, 0, 255).astype(np.uint8)

        return image

    def _preprocess(self, image: np.ndarray) -> np.ndarray:
        """Preprocess image to match NanoDet format.

        Args:
            image: (H, W, 3) uint8 RGB.

        Returns:
            Preprocessed tensor (3, H, W) float32 normalized.
        """
        # Convert RGB to BGR (NanoDet uses BGR)
        bgr = image[..., ::-1].copy()

        # Normalize with ImageNet stats
        normalized = (bgr.astype(np.float32) - MEAN) / STD

        # Convert to CHW format
        return normalized.transpose(2, 0, 1)


def create_dataloaders(
    data_path: str,
    batch_size: int = 64,
    num_workers: int = 4,
    roi_size: int = 40,
) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:
    """Create train and validation dataloaders.

    Args:
        data_path: Path to dataset directory.
        batch_size: Batch size.
        num_workers: Number of data loading workers.
        roi_size: Expected ROI size.

    Returns:
        Tuple of (train_loader, val_loader).
    """
    train_dataset = TinyDroneDataset(
        data_path, split="train", augment=True, roi_size=roi_size
    )
    val_dataset = TinyDroneDataset(
        data_path, split="val", augment=False, roi_size=roi_size
    )

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
    )

    return train_loader, val_loader
